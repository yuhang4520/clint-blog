---
title: vercel ai-sdk 项目接入本地ai模型方案，踩坑，思考
date: '2025-07-01'
tags: ['Ai', 'next', 'vercel']
draft: false
summary: 项目总体介绍以及开发计划
---

# 模型本地化方案

犹豫技术选型是基于vercel ai-sdk项目为模板，为了继续使用useChat等Hooks处理消息流、输入框状态和渲染逻辑，也为了后面替换模型统一封装，方案的最终选择也会考虑与模板的适配度
(暂未找到接入 vercel ai-sdk 的本地模型方案，未确定是否可行)

## 方案一

**使用transformers.js 加载本地量化模型文件（ONNX）**

- 优势：轻量化，硬件要求低，可集成打包
- 劣势：ONNX格式资源少，安装包大，**兼容难度大** （接入ai-sdk模板，想兼容原有api未找到相关文档，）
- 模板适配度：⭐️⭐️
- 思考：是否可用方案二的思路，是否可在api层面进行兼容？

## 方案二

**Electron主进程启用Ollama服务加载本地模型 **

- 优势： 可使用GGUF文件，模型选择广，​​RESTful API交互
- 劣势：需要用户预装Ollama，集成度不够高
- 模板适配度：⭐️⭐️⭐️⭐️
- 思考：预装Ollama 是否可实现，安装后下载模型也需要网络，与设计初衷完全断网运行相违背

## 方案三

**​​Electron主进程启用Ollama服务加载本地模型**

- 优势：高性能C++推理框架，适合资源受限环境，可集成打包，（node-llama-cpp）
- 劣势：安装包巨大（是否可实现光盘安装），需要进程隔离，可能涉及底层修改
- 模板适配度：未知
